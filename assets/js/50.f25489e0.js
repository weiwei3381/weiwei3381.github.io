(window.webpackJsonp=window.webpackJsonp||[]).push([[50],{414:function(t,a,s){"use strict";s.r(a);var n=s(19),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"深度学习与实践"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#深度学习与实践"}},[t._v("#")]),t._v(" 深度学习与实践")]),t._v(" "),s("h2",{attrs:{id:"numpy常用api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#numpy常用api"}},[t._v("#")]),t._v(" numpy常用API")]),t._v(" "),s("h2",{attrs:{id:"torch常用api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#torch常用api"}},[t._v("#")]),t._v(" torch常用API")]),t._v(" "),s("p",[t._v("torch CPU版本安装使用"),s("code",[t._v("pip install torch")]),t._v("即可.")]),t._v(" "),s("h2",{attrs:{id:"hugging-face实战"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hugging-face实战"}},[t._v("#")]),t._v(" Hugging Face实战")]),t._v(" "),s("h3",{attrs:{id:"transformer各种任务介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformer各种任务介绍"}},[t._v("#")]),t._v(" transformer各种任务介绍")]),t._v(" "),s("h3",{attrs:{id:"常用api"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常用api"}},[t._v("#")]),t._v(" 常用API")]),t._v(" "),s("h3",{attrs:{id:"pipeline"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pipeline"}},[t._v("#")]),t._v(" Pipeline")]),t._v(" "),s("p",[t._v("Pipeline可以很方便的使用huggingface提供的NLP任务, 示例代码如下:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 导入模块")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pipeline\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 指定pipeline的任务, 指定后, pipeline会自动从huggingface上下载相应的模型并加载, 并返回一个指定任务的Pipeline对象")]),t._v("\nsummarizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"summarization"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("其中, pipeline的第一个参数为"),s("code",[t._v("task: str = None")]),t._v(", 可选的值有很多, 可参考"),s("a",{attrs:{href:"https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline.task",target:"_blank",rel:"noopener noreferrer"}},[t._v("文档"),s("OutboundLink")],1),t._v(", 其中常用值有:")]),t._v(" "),s("ul",[s("li",[t._v('"summarization": 文本摘要')]),t._v(" "),s("li",[t._v('"text2text-generation": 文本到文本生成')]),t._v(" "),s("li",[t._v('"text-classification"或者"sentiment-analysis": 文本分类')]),t._v(" "),s("li",[t._v('"text-generation": 文本生成')]),t._v(" "),s("li",[t._v('"token-classification"或者"ner": 命名实体识别')]),t._v(" "),s("li",[t._v('"translation": 翻译')])]),t._v(" "),s("p",[t._v("pipeline的第二个参数为"),s("code",[t._v("model: str|PreTrainedModel | TFPreTrainedModel")]),t._v(", 该参数为可选值, 可传入指定模型, 如果是str类型, 则会加载本地或者自动从huggingface网站上下载对应名称的模型并加载, 因此本地代码可以写成:")]),t._v(" "),s("p",[s("code",[t._v("summarizer=pipeline('summarization','mypath/distilbart-cnn-12-6')")])]),t._v(" "),s("p",[t._v("由于model入参也可以直接以PreTrainedModel对象的形式传入，tokenizer也可以直接以PreTrainedTokenizer对象的形式传入。这种传入形式的写法示例：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoModelForSeq2SeqLM\n\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mypath/distilbart-cnn-12-6"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModelForSeq2SeqLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mypath/distilbart-cnn-12-6"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsummarizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'summarization'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("tokenizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("比较麻烦的是AutoClass类的选择, 在本文摘要里面使用的Auto类为"),s("code",[t._v("AutoModelForSeq2SeqLM")]),t._v(", 可以在huggingface上点击右上角的【</>Use in Transformers】按钮，如下图所示。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic.imgdb.cn/item/63f246b9f144a01007611cea.jpg",alt:"查看transformers代码"}})]),t._v(" "),s("p",[t._v("代码返回的是一个"),s("code",[t._v("SummarizationPipeline")]),t._v("对象, 使用的话也很简单"),s("code",[t._v("summarizer(str_list: 需要摘要的文本列表, min_length=5, max_length=20)")]),t._v("即可.")]),t._v(" "),s("h3",{attrs:{id:"transformer模型使用-文本生成任务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformer模型使用-文本生成任务"}},[t._v("#")]),t._v(" transformer模型使用——文本生成任务")]),t._v(" "),s("h4",{attrs:{id:"使用现有的模型生成文本"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用现有的模型生成文本"}},[t._v("#")]),t._v(" 使用现有的模型生成文本")]),t._v(" "),s("p",[t._v("在"),s("a",{attrs:{href:"https://huggingface.co/",target:"_blank",rel:"noopener noreferrer"}},[t._v("抱抱脸（huggingface）"),s("OutboundLink")],1),t._v("上开源的small或者tiny版本的中文NLG(nature language generate, 自然语言生成)模型主要有下列几个。")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://huggingface.co/bigscience/bloom-560m",target:"_blank",rel:"noopener noreferrer"}},[t._v("bigscience/bloom-560m"),s("OutboundLink")],1),t._v("，2022年5月26日由BigScience公司推出，模型大小1.12GB，支持48种语言。")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://huggingface.co/IDEA-CCNL/Wenzhong-GPT2-110M",target:"_blank",rel:"noopener noreferrer"}},[t._v("IDEA-CCNL/Wenzhong-GPT2-110M"),s("OutboundLink")],1),t._v(', "闻仲"1.0版本, 属于IDEA 研究院"封神榜"开源模型系列, 2022年5月上传至huggingface, 模型大小274MB。')]),t._v(" "),s("li",[s("a",{attrs:{href:"https://huggingface.co/IDEA-CCNL/Wenzhong2.0-GPT2-110M-BertTokenizer-chinese",target:"_blank",rel:"noopener noreferrer"}},[t._v("IDEA-CCNL/Wenzhong2.0-GPT2-110M-BertTokenizer-chinese"),s("OutboundLink")],1),t._v(', "闻仲"2.0版本, 属于IDEA 研究院"封神榜"开源模型系列2.0版本, 基于BertTokenizer，实现字级别token，2022年12月上传至huggingface, 模型大小为421MB。')]),t._v(" "),s("li",[s("a",{attrs:{href:"https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall",target:"_blank",rel:"noopener noreferrer"}},[t._v("uer/gpt2-chinese-cluecorpussmall"),s("OutboundLink")],1),t._v("，通用GPT2中文小模型, 2021年上传至huggingface, 模型大小为421MB。")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall",target:"_blank",rel:"noopener noreferrer"}},[t._v("uer/gpt2-distil-chinese-cluecorpussmall"),s("OutboundLink")],1),t._v("，蒸馏后的GPT2中文小模型，2021年上传至huggingface，模型大小为244MB。")])]),t._v(" "),s("p",[t._v("运行上述模型主要采用huggingface上的transformer模块进行，"),s("a",{attrs:{href:"https://huggingface.co/docs/transformers/main/zh/index",target:"_blank",rel:"noopener noreferrer"}},[t._v("官方中文文档"),s("OutboundLink")],1),t._v("很详细。")]),t._v(" "),s("p",[t._v("使用"),s("code",[t._v("pip install torch transformers numpy")]),t._v("安装必要模块")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("GPT2LMHeadModel\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" warnings\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 不显示警告信息")]),t._v("\nwarnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filterwarnings"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ignore"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型名称, 这里采用开源的闻仲GPT2 110M参数版本, 文件大小为421MB")]),t._v("\nmodel_path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'IDEA-CCNL/Wenzhong2.0-GPT2-110M-BertTokenizer-chinese'")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建分词器, 本地不存在则会从远程下载,这个文件不大,一般包括vocab.txt, tokenizer_config.json, 默认存储位置在C:\\Users\\用户名\\.cache\\huggingface\\hub下")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建模型, 比较大, 有421MB")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" GPT2LMHeadModel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获得文本填充的后续词")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("generate_word_level")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_return"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("top_p"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    inputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("add_special_tokens"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    gen "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                            inputs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("max_length"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            do_sample"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            top_p"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("top_p"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            eos_token_id"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("21133")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            pad_token_id"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            num_return_sequences"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("n_return"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    sentences "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch_decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sentence "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentences"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'sentence ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(": ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-----'")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" gen\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 开始测试")]),t._v("\noutputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" generate_word_level"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'实践出真知，实践长才干。'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_return"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"huggingface模型转为onnx格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#huggingface模型转为onnx格式"}},[t._v("#")]),t._v(" huggingface模型转为onnx格式")]),t._v(" "),s("p",[t._v("huggingface提供了三种方式进行转换, 不过我测试之后只在一个代码上运行成功了.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" optimum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("onnxruntime "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ORTModelForSequenceClassification\n\nmodel_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'IDEA-CCNL/Wenzhong2.0-GPT2-110M-BertTokenizer-chinese'")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 相当于使用ORTModel的方式进行模型加载, 来源transformer, 其中参数设置为from_transformers=True")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 运行代码成功之后, 用everything在全盘搜索"*.onnx", 最新生成的就是转换后的onnx模型')]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ORTModelForSequenceClassification"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_transformers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"huggingface模型微调"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#huggingface模型微调"}},[t._v("#")]),t._v(" huggingface模型微调")]),t._v(" "),s("h2",{attrs:{id:"深度学习本地实战"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#深度学习本地实战"}},[t._v("#")]),t._v(" 深度学习本地实战")]),t._v(" "),s("h3",{attrs:{id:"本地部署一键抠图"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#本地部署一键抠图"}},[t._v("#")]),t._v(" 本地部署一键抠图")]),t._v(" "),s("p",[t._v("一键抠图使用的是python库"),s("a",{attrs:{href:"https://github.com/plemeri/transparent-background",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transparent Background"),s("OutboundLink")],1),t._v("，不支持3.7及以下版本，建议使用3.10版本的python")]),t._v(" "),s("p",[t._v("1.安装v3.10.10的64位python"),s("a",{attrs:{href:"https://www.python.org/ftp/python/3.10.10/python-3.10.10-amd64.exe",target:"_blank",rel:"noopener noreferrer"}},[t._v("下载地址"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("2.安装必须的依赖库")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# CPU 版本：")]),t._v("\npip3 install torch torchvision torchaudio "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("i https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("mirrors"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aliyun"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("pypi"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("simple\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# GPU 版本：")]),t._v("\npip3 install torch torchvision torchaudio "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("url https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("download"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pytorch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("org"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("whl"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cu118\n")])])]),s("p",[t._v("3.安装 Transparent BG，使用命令"),s("code",[t._v("pip3 install transparent-background -i https://mirrors.aliyun.com/pypi/simple")])]),t._v(" "),s("p",[t._v("4.下载模型和一键发送的批处理文件，"),s("a",{attrs:{href:"https://pan.baidu.com/s/10yZfxegIqgl9V3QvH4k0EA?pwd=n49d",target:"_blank",rel:"noopener noreferrer"}},[t._v("百度网盘地址"),s("OutboundLink")],1),t._v("，下载后解压"),s("code",[t._v("TransparentBG_Win.7z")]),t._v("文件，然后双击"),s("code",[t._v("开始.bat")]),t._v("文件安装快捷方式。")]),t._v(" "),s("p",[t._v("5.使用方法，支持png和jpg格式的图片，对着图片文件点击右键，选择“发送到-一键抠图”，如下图所示，抠图完成后会自动在原目录生成抠图后的图片。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://pic.imgdb.cn/item/65f053b39f345e8d032a1fd6.png",alt:"一键抠图"}})])])}),[],!1,null,null,null);a.default=e.exports}}]);